**Искусственный интеллект. Модель, проектирование, разработка.**




**Автор проекта: Самонин Евгений Игоревич, 10 класс**

**2025**
#
**ОГЛАВЛЕНИЕ**

[ВВЕДЕНИЕ	3](#_toc190794883)

[1.	ТЕОРЕТИЧСЕКИЕ ОСНОВЫ ПРОЕКТА	4](#_toc190794884)

[1.1	История создания нейронных сетей	4](#_toc190794885)

[1.2	Современные нейронные сети	5](#_toc190794886)

[1.3	Понятие нейронной сети в современном мире	7](#_toc190794887)

[1.4	Основные выполняемые нейросетями функции	8](#_toc190794888)

[1.5	Ключевые для понимания определения	8](#_toc190794889)

[1.6	Алгоритм работы	9](#_toc190794890)

[1.6.1	слои перцептрона	9](#_toc190794891)

[1.6.2	Вычисление предсказания нейронной сети	9](#_toc190794892)

[1.6.3	Обучение нейронной сети	11](#_toc190794893)

[2	ПРАКТИЧЕСКАЯ ЧАСТЬ	15](#_toc190794894)

[2.1	Код модели нейронной сети	15](#_toc190794895)

[2.2	Обучение нейронной сети	17](#_toc190794896)

[2.2.1	Эффективность работы	17](#_toc190794897)

[2.2.2	Скорость работы	19](#_toc190794898)

[ЗАКЛЮЧЕНИЕ	20](#_toc190794899)

[БИБЛИОГРАФИЯ	22](#_toc190794900)

[ПРИЛОЖЕНИЕ	23](#_toc190794901)
#
<a name="_toc190794883"></a>**ВВЕДЕНИЕ**

**Технологии искусственного интеллекта призваны стать важнейшим ресурсом достижения национальных целей развития** 

Президент РФ Путин В.В. [[12](https://tass.ru/ekonomika/22643091)]

Проектирование и разработка искусственного интеллекта в современности — одна из наиболее быстро развивающихся и перспективных областей как бизнеса, так и науки. В реалиях скачка компьютерных технологий и глобализации, ИИ становится универсальным помощником для достижения преимуществ перед конкурентами и инноваций в различных направлениях экономики.

**Актуальность** проектной работы, определяется тем, что искусственный интеллект используется во многих сферах нашей жизни и позволяет увеличить продуктивность, как в творческих задачах, так и в задачах требующих обработки большого количества данных. В образовательной сфере ИИ открывает новые возможности для личного обучения и улучшения образовательных программ.

Работая над этим проектом, я поставил перед собой **цель:** создать модель нейронной сети. 

Для достижения этой цели, требуется решить следующие **задачи**:

- Изучить историю создания искусственного интеллекта.
- Проанализировать современные нейронные сети.
- Рассмотреть алгоритм работы полносвязного перцептрона.
- Создать свою модель нейронной сети.

**Объект:** Искусственный интеллект.

**Предмет:**  Модель нейронной сети. 

**Гипотеза:** Использование искусственных нейронных сетей позволяет автоматизировать задачи классификации данных, что повышает точность и скорость обработки информации по сравнению с ручными методами.

Для решения этих задач в ходе работы применены **методы исследования**:

- Информационно-поисковый (теоретический) метод: анализ источников.
- Практический метод: моделирование, эксперимент.
#
1 <a name="_hlk180604461"></a><a name="_toc190794884"></a>**ТЕОРЕТИЧСЕКИЕ ОСНОВЫ ПРОЕКТА**
1.1 <a name="_toc190794885"></a>**История создания нейронных сетей**

Изначально люди пытались воссоздать работу мозга человека с помощью компьютера и для этого выдвинули модели нейронов и связей между ними:

В 1943-м Уорреном Мак-Каллоком и Уолтером Питтсом была предложена математическая модель нейрона, а уже в конце 1950-х Фрэнк Розенблатт представил персептрон – простую модель машинного обучения, созданную для помощи компьютерам в обучении на разных объёмах данных. Именно его можно назвать первой практической реализацией нейросети. [[4](https://habr.com/ru/articles/861888/)]

Именно для этого сначала использовать бинарная система активации нейрона. Либо нейрон активирован, либо нет. После этого многие поняли ограниченность нейронных сетей и настала «Зима искусственного интеллекта». Но позже с ростом сложности алгоритмов нейронных сетей, а также вычислительных возможностей компьютеров интерес появился вновь. В это время начали появляться способы «глубокого» обучения сетей, например обратное распространение ошибки, именно этот способ рассмотрен в этом проекте.

В период 1980-2000 стали разрабатываться и появляться первые алгоритмы обучения, сравнения и анализа данных, в XXI направление стало стремительно развиваться. В 2000-х появились мощные графические процессоры, и стали доступны большие объёмы данных, что побудило сообщество начать разработку алгоритмов Deep Learning – совокупность методов машинного обучения, основанных на обучении представлениям, а не на специализированных алгоритмах под конкретные задачи. [[4](https://habr.com/ru/articles/861888/)]

В современном времени бум популярности нейронных сетей пришел с выходом Chat-GPT 3, который был способен осознанно отвечать на вопросы пользователя. После чего каждая крупная компания стала ставить перед собой цель конкурирования на рынке искусственного интеллекта, благодаря этому нейронные сети получили свое развитие и в современности достигают новых успехов изо дня в день.

  1.2 <a name="_toc190794886"></a>**Современные нейронные сети**

Благодаря современным вычислительным мощностям, новым архитектурам и количеству данных для обучения, нейронные сети стали не только машинами для предсказания и классификации, но и помощниками для обычных людей. В этом блоке будут рассмотрены современные нейронные сети, которые могут помогать обывателю без знания их устройства и примеры использования нейронных сетей крупными компаниями. 

Нейросети для обработки текста:

- Первооткрыватель и главный двигатель прогресса в мире ИИ — Chat GPT. Мощнейший искусственный интеллект способный обрабатывать естественный язык, аудио, видео контент и различные документы. Он способен сделать за вас текст любой стилистики и на любую тему.
- Google Gemini. Чат бот с доступом к режиму разговорного диалога и обработки фотографий. Имеет бесплатный API для интеграции в свои проекты.
- DeepSeek. Полностью бесплатный чат бот с открытым исходным кодом, что позволяет локально установить его на свой компьютер и пользоваться без отправки своих данных в сеть.

Нейросети для обработки изображений:

- DALL E (OpenAI) — это одна из самых известных нейросетей для генерации изображений на основе текстовых описаний. Она способна создавать уникальные изображения. DALL E интегрирована с ChatGPT, что позволяет получать более точные результаты.
- MidJourney — это инструмент для создания художественных изображений. Нейросеть особенно популярна среди художников и дизайнеров благодаря своей способности генерировать изображения с уникальным стилем.
- Российские разработки: Шедеврум и Kandinsky от Яндекса и Сбера для генерации изображений по описанию.

Нейросети для обработки аудио:

- OpenAI Whisper — это нейросеть, предназначенная для распознавания и преобразования речи в текст. Она поддерживает множество языков и диалектов, а также способна работать с шумными аудиозаписями. Но, что является и плюсом, и минусом, доступна только либо для скачивания, либо через API.
- ElevenLabs — это нейросеть для генерации реалистичной речи. Она позволяет создавать аудио с различными голосами, настроением и интонацией. ElevenLabs часто используется для озвучки видео, создания аудиокниг и даже для разработки голосовых помощников. Нейросеть поддерживает API для интеграции в сторонние приложения.

Нейросети для анализа данных:

- TensorFlow — это одна из самых популярных платформ для машинного обучения и анализа данных. Она поддерживает создание и обучение нейронных сетей для решения многих задач. TensorFlow широко используется в научных исследованиях и промышленных приложениях.
- PyTorch — это** готовый набор инструментов для машинного обучения, который особенно популярен среди исследователей. PyTorch активно используется в задачах компьютерного зрения и обработки естественного языка.

Так с помощью этих нейросетей вы можете сделать свою жизнь проще. Они могут стать личным учителем, который может рассказать вам, что угодно и когда угодно или объяснить непонятную тему. Так и помочь учителю составить план урока, презентацию или проверить работы.  

Нейросети в большом бизнесе:

- Машины на автопилоте. Дальше всех в этой сфере продвинулась компания «Tesla». В ее машинах установлены мощные камеры и компьютер способный обрабатывать тысячи изображений в секунду и принимать решение на их основе.
- Алгоритмы рекомендаций больших компаний для выдачи таргетированной лично на вас информации, например в YouTube, Spotify, Netflix.

1.3 <a name="_toc190794887"></a>**Понятие нейронной сети в современном мире**

Рассмотрим определение термина нейронной сети и ИИ.

Иску́сственный интелле́кт (ИИ) (англ. artificial intelligence; AI) в самом широком смысле — это интеллект, демонстрируемый машинами, в частности компьютерными системами. Это область исследований в области компьютерных наук, которая разрабатывает и изучает методы и программное обеспечение, позволяющие машинам воспринимать окружающую среду и использовать обучение и интеллект для выполнения действий, которые максимально увеличивают их шансы на достижение поставленных целей. Такие машины можно назвать искусственным интеллектом. ИНС представляет собой систему соединённых и взаимодействующих между собой простых процессоров (искусственных нейронов). Такие процессоры довольно просты (особенно в сравнении с процессорами, используемыми в персональных компьютерах). Каждый процессор подобной сети имеет дело только с сигналами, которые он периодически получает, и сигналами, которые он периодически посылает другим процессорам. И, тем не менее, будучи соединёнными в достаточно большую сеть с управляемым взаимодействием, такие простые по отдельности процессоры вместе способны выполнять довольно сложные задачи [[7](https://ru.wikipedia.org/wiki/ИИ)].

Нейронная сеть (также искусственная нейронная сеть, ИНС, или просто нейросеть) — математическая модель, а также её программное или аппаратное воплощение, построенная по принципу организации биологических нейронных сетей — сетей нервных клеток живого организма. Это понятие возникло при изучении процессов, протекающих в мозге, и при попытке смоделировать эти процессы. Первой такой попыткой были нейронные сети У. Маккалока и У. Питтса. После разработки алгоритмов обучения получаемые модели стали использовать в практических целях: в задачах прогнозирования, для распознавания образов, в задачах управления и др. [[8](https://ru.wikipedia.org/wiki/Нейронная_сеть)]. 

Если проанализировать эти определения, то можно понять, что  искусственный интеллект — это то, что демонстрирует обученная нейронная сеть, и что напоминает интеллект человека. А  нейронная сеть — это математическая модель, построенная на основе принципа работы человеческого мозга и выполняющая конкретные функции.

  1.4 <a name="_toc190794888"></a>**Основные выполняемые нейросетями функции**
- Классификация — распределение данных по классам на основе определённых параметров. Например, на вход дается набор характеристик человека и от нейросети требуется понять: давать человеку кредит или нет.
- Предсказание — возможность анализировать закономерности и на их основе давать предсказания. Например, рост или падение акций, основываясь на ситуации на фондовом рынке.
- Распознавание — Идентификация и детектирование объектов на изображении.

В этом проекте я буду рассматривать устройство классифицирующей нейросети для дальнейшего ее создания.

  1.5 <a name="_toc190794889"></a>**Ключевые для понимания определения**

СПР — сеть прямого распространения.

Искусственный нейрон или просто нейрон — это составляющая часть слоя нейронной сети, обладающая связями (синапсами) с нейронами другого слоя, сумматор для суммирования всех подающихся на вход значений и смещение.

Синапс — связь между нейронами разных слоев ИНС, имеющая вес.

Полносвязный перцептрон — искусственная нейронная сеть, состоящая из слоев, последовательно связанных друг с другом.

Гиперпараметры нейросети — это параметры, которые не меняются в процессе обучения нейросети и подбираются вручную разработчиком исходя из вычислительных мощностей и потребностей.

Весы ИНС — все значения связей и смещений (все обучаемые характеристики).

Батч (batch) — неполная подвыборка из всего массива данных. 

1.6 <a name="_toc190794890"></a>**Алгоритм работы**

В этом проекте будет рассмотрен принцип работы полносвязного перцептрона — одного из типов нейронных сетей из-за его простоты и наглядности для понимания работы нейросети. Он помогает понять основы устройства искусственного интеллекта, так как полносвязные слои, из которых он состоит, присутствуют почти во всех типах нейронных сетей.

1.6.1 <a name="_toc190794891"></a>**слои перцептрона**

Проанализировав источники [[11](https://neural.radkopeter.ru/chapter/персептроны/)] [[6](https://habr.com/ru/articles/214317/)] и то, как сам Розенблатт предложивший идею перцептрона, говорил о их устройстве, можно понять, что слои делятся на три типа:

1\. Входной — нейроны, на которые изначально подается входная информация, будь это слова текста или пиксели фотографии. Входной слой всегда один. Входных нейронов в слое должно быть столько же, сколько у нас есть входных данных. Например, если на вход подается изображение разрешением 28 на 28 пикселей и каждый пиксель закодирован одним числом, то входных нейронов должно быть 28 \* 28 = 784. Значение входного нейрона никак не обрабатывается и передается следующим слоям неизменным. 

2\. Скрытые слои — нейроны, находящиеся между входными и выходными, слоев этих нейронов может быть сколько угодно. Пользователь никак не контактирует с этими нейронами, поэтому они и называются скрытыми. Количество нейронов в скрытых слоях выбирается заранее разработчиком. Нейроны скрытых слоев берут свое значение исходя из значений всех нейронов предыдущего слоя и передают его на следующий слой.

3\. Выходной слой — последний слой нейронов, дающий окончательный результат. Выходной слой тоже только один. Нейронов в этом слое столько же, сколько возможных вариантов ответа. Ответом является нейрон с наибольшим значением.

1.6.2 <a name="_hlk190380957"></a><a name="_toc190794892"></a>**Вычисление предсказания нейронной сети**

Начнем с того, как нейрон преобразует переданную ему информацию. Как стало понятно из вышесказанного, нейроны входного слоя ничего не делают с этой информацией, тогда нужно понять, как это делают нейроны скрытых и выходного слоев.

Синапс — это связь между двумя нейронами, которая имеет вес. У каждого нейрона одного слоя есть синапсы с каждым нейроном предыдущего и последующего слоя. И так каждый нейрон передает свое значение каждому следующему нейрону, умножая его на вес конкретного синапса. Эта информация поступает на нейрон и суммируется с также полученными значениями других нейронов, после чего к этой сумме прибавляется смещение нейрона. Дальше эту сумму нужно нормализовать, иначе предсказание сети будет линейной функцией неспособной к сложному анализу закономерностей, для этого и используется функция нормализации, в моем случае я буду использовать функцию ReLU, так как она используется чаще всего и требует меньше вычислительных мощностей в отличие от остальных функций активации [[10](https://deepmachinelearning.ru/docs/Neural-networks/MLP/Activation-functions)].

ReLU (rectified linear unit) — это функция, превращающая отрицательные значения в 0, а положительные оставляет как есть. [[3](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))].

Формула ReLU:

ReLU(x)= max(0, x)

График функции ReLU: 
![График функции ReLU](https://i.postimg.cc/d1PqR70r/Re-LU.png)

Для этого и нужно смещение, чтобы задать больше, чего должна быть сумма значений предыдущих нейронов, например, чтобы нейрон активировался, только если сумма больше 10 или больше -100.

После нормализации значения сигнал передается дальше по синапсам в следующие слои.

Рассмотрим всю эту схему на примере с двумя входными нейронами и одним скрытым нейроном. Обозначим входные нейроны I<sub>1</sub>, I<sub>2</sub>, скрытый как N, веса синапсов как W и смещение B. Тогда входными и выходными данными на нейрон будут:

<i>N<sub>input</sub> = I<sub>1</sub>*W<sub>1</sub> + I<sub>2</sub>*W<sub>2</sub></i>

<i>N<sub>output</sub> = ReLU(I<sub>1</sub>*W<sub>1</sub> + I<sub>2</sub>*W<sub>2</sub> + B)</i>

Запишем всё это в алгебраическом виде и сделаем формулы общими. Представим n-ный (отсчет идет с нуля) нейрон L-ого слоя (кроме входного и выходного) нейронной сети, обозначим A<sup>L</sup><sub>n</sub>, нейроны предыдущего слоя A<sup>L-1</sup><sub>k</sub>, весы синапсов как W<sub>k, n</sub>, а смещение как B<sub>n</sub>, а, где k— это номер нейрона предыдущего слоя, а n— это номер рассматриваемого нейрона. Значение этого нейрона: 
A = ReLU(W<sub>0,0</sub>\*A<sup>L-1</sup><sub>0</sub> + W<sub>1,0</sub>\*A<sup>L-1</sup><sub>1</sub>+ ... + W<sub>n, 0</sub>\*A<sup>L-1</sup><sub>k</sub> + B<sub>n</sub>) 
Если рассмотреть эту формулу без смещения и ReLU поподробнее, можно понять, что это первый элемент вектора, получаемого при умножении вектора значений нейронов предыдущего слоя и матрицы весов (каждая строка — это вектор весов синапсов нейрона предыдущего слоя со всеми нейронами текущего) = W<sub>0,0</sub>\*A<sub>0</sub> + W<sub>0,1\*</sub>A<sub>1</sub> + ... + W<sub>0,n</sub>\*A<sub>n</sub>. Тогда, чтобы получить наше значение, нужно к этому произведению прибавить вектор смещений и взять сигмоиду. И все это можно красиво записать для первого слоя: A<sup>1</sup> = ReLU(WA<sup>0</sup> + B) или в общем виде:

[![\\ A^L=\ ReLU\left(\left[\begin{matrix}A_0^{L-1}&\cdots&A_k^{L-1}\\\end{matrix}\right]\left[\begin{matrix}W_{0,\ 0}&\cdots&W_{0,n}\\\vdots&\ddots&\vdots\\W_{k,0}&\cdots&W_{k,n}\\\end{matrix}\right]+\left[\begin{matrix}B_1\\\vdots\\B_n\\\end{matrix}\right]\right)=\ ReLU(A^{L-1}\ast W+B)](https://latex.codecogs.com/svg.latex?%5C%5C%20A%5EL%3D%5C%20ReLU%5Cleft(%5Cleft%5B%5Cbegin%7Bmatrix%7DA_0%5E%7BL-1%7D%26%5Ccdots%26A_k%5E%7BL-1%7D%5C%5C%5Cend%7Bmatrix%7D%5Cright%5D%5Cleft%5B%5Cbegin%7Bmatrix%7DW_%7B0%2C%5C%200%7D%26%5Ccdots%26W_%7B0%2Cn%7D%5C%5C%5Cvdots%26%5Cddots%26%5Cvdots%5C%5CW_%7Bk%2C0%7D%26%5Ccdots%26W_%7Bk%2Cn%7D%5C%5C%5Cend%7Bmatrix%7D%5Cright%5D%2B%5Cleft%5B%5Cbegin%7Bmatrix%7DB_1%5C%5C%5Cvdots%5C%5CB_n%5C%5C%5Cend%7Bmatrix%7D%5Cright%5D%5Cright)%3D%5C%20ReLU(A%5E%7BL-1%7D%5Cast%20W%2BB))](#_)

L – номер слоя, n – количество нейронов в новом слое, k – количество нейронов в предыдущем слое. Сами формулы и как их выводить я изучил на нескольких источниках для подтверждения достоверности. [[2](https://habr.com/ru/articles/722422/#anchor_3)] [[9](https://education.yandex.ru/handbook/ml/article/pervoe-znakomstvo-s-polnosvyaznymi-nejrosetyami#forward-and-backward-propagation)] 

Так применяя эту формулу для каждого слоя, нейросеть  доходит  до последнего, а в нем нужно использовать другую функцию. Функция Softmax преобразует данный ей вектор в вектор вероятностей, так чтобы сумма этих вероятностей была равна единице. Её формула для каждого члена вектора:

[![\\ Softmax(y_i)\ =\ \frac{e^{y_i}}{\sum_{i=0}^{n}e^{y_i}}](https://latex.codecogs.com/svg.latex?%5C%5C%20Softmax(y_i)%5C%20%3D%5C%20%5Cfrac%7Be%5E%7By_i%7D%7D%7B%5Csum_%7Bi%3D0%7D%5E%7Bn%7De%5E%7By_i%7D%7D)](#_)

Так ответом нейросети является распределение вероятностей ответов.

1.6.3 <a name="_toc190794893"></a>**Обучение нейронной сети**

Нейросеть учится на данных. Эти данные должны быть заранее заготовлены и сгруппированы. Дальше эти данные делятся на два типа: тренировочные и тестовые. Тренировочные используются напрямую при обучении. Тестовые используются для отслеживания динамики обучения.

Обучение нейронной сети происходит по этапам, каждый этап называется эпохой. Данные для обучения для каждой эпохи одинаковые, и эпохой и называется одна итерация обучения на базе данных. Для каждой эпохи желательно каждый раз перемешивать данные. В каждой эпохе происходят следующие действия: 

1. Деление данных на батчи (размер батча — это гиперпараметр). 
1. Прямое распространение, т. е. получение результатов предсказания нейросети для этих данных.
1. Обратное распространение ошибки — это способ получения изменений весов ИНС.
1. Корректировка значений весов на основе суммарной ошибки за весь батч.

И далее те же действия по итерациям батчей и эпох.

Рассмотрим обучение способом стохастического градиентного спуска. В математике градиентом функции называют «направление» в пространстве функции, в котором она возрастает. Для нейросети это будет функция ошибки. Она показывает, насколько предсказание нейросети отличается от действительности. Так как функцию ошибки необходимо минимизировать, брать градиент нужно со знаком минус. Стохастическим его называют, потому что обучение идет по случайным неполным частям всех данных, а спуском из-за того, что, уменьшая функцию ошибки ее значение опускается в локальный минимум, где и остается.
![Пример градиентного спуска](https://i.postimg.cc/j2Rt9GPj/image.png)
На изображении показан случай с 2-х мерной функцией и одним глобальным минимумом, но в действительности у функции ошибки в нейросети параметров, а следовательно, и пространств сотни тысяч и множество локальных минимумов. Для получения вектора градиента используют метод обратного распространения ошибки. Он называется так, потому что мы сначала получаем градиент по последнему слою, редактируем слой, а после передаем с помощью формул этот градиент на предыдущий слой, и всё повторяется циклично.

Рассмотрев формулы обратного распространения ошибки из разных источников [[1](https://habr.com/ru/companies/ods/articles/344116/)] [[5](https://en.wikipedia.org/wiki/Backpropagation)] и проанализировав их, можно записать их в матричном виде для функции активации RuLU для скрытх слоев и softmax для послднего и crossentropy как функцию ошибки. Пусть L это номер слоя, T это транспонированная версия вектора или матрицы, W<sup>L</sup> это матрица весов между слоями L и L – 1, B<sup>L</sup> это вектор смещений данного слоя, Z то, что передается в функцию активации, F это функция активации, ⊙** это поэлементное умножение, A<sup>L</sup> это вектор значений слоя L. То на сколько нужно изменить вес обозначается производной ошибки по этому весу, но я буду рассматривать формулы относительно матриц и векторов целых слоев для большей производительности программы.

[![\\ \frac{dE}{dZ^L}=\ \frac{dE}{dA^L}\odot F^\prime\left(Z^L\right)\ \ \ \frac{dE}{dW^L}={{(A}^{L-1})}^T\ast\frac{dE}{dZ^L}\ \ \ \frac{dE}{dB^L}=\ \frac{dE}{dZ^L}\ \ \ \frac{dE}{dA^{L-1}}=\ \frac{dE}{dZ^L}\ast{(W^L)}^T](https://latex.codecogs.com/svg.latex?%5C%5C%20%5Cfrac%7BdE%7D%7BdZ%5EL%7D%3D%5C%20%5Cfrac%7BdE%7D%7BdA%5EL%7D%5Codot%20F%5E%5Cprime%5Cleft(Z%5EL%5Cright)%5C%20%5C%20%5C%20%5Cfrac%7BdE%7D%7BdW%5EL%7D%3D%7B%7B(A%7D%5E%7BL-1%7D)%7D%5ET%5Cast%5Cfrac%7BdE%7D%7BdZ%5EL%7D%5C%20%5C%20%5C%20%5Cfrac%7BdE%7D%7BdB%5EL%7D%3D%5C%20%5Cfrac%7BdE%7D%7BdZ%5EL%7D%5C%20%5C%20%5C%20%5Cfrac%7BdE%7D%7BdA%5E%7BL-1%7D%7D%3D%5C%20%5Cfrac%7BdE%7D%7BdZ%5EL%7D%5Cast%7B(W%5EL)%7D%5ET)](#_)

После чего dE/dA<sup>L-1</sup> становится dE/dA<sup>L</sup> для предыдущего слоя и все повторяется по итерациям. Но для первой итерации нужно знать dE/dZ<sup>L</sup> последнего слоя. Для этого есть отдельная формула. Для нее введём еще несколько обозначений. Y это правильный результат, SoftMax(Z<sup>max</sup>) = P (результат работы нейросети в вероятностях).

[![\\ \frac{dE}{dZ^L}=P-Y](https://latex.codecogs.com/svg.latex?%5C%5C%20%5Cfrac%7BdE%7D%7BdZ%5EL%7D%3DP-Y)](#_)

Но так как нейронная сеть работает не с единичными прогнозами, а с батчами, то формулы изменятся. Из-за свойств матричного умножения почти все формулы останутся теми же, кроме одной: 

[![\\ \frac{dE}{dB^L}=\sum_{j}\frac{dE_j}{d{Z_j}^L}](https://latex.codecogs.com/svg.latex?%5C%5C%20%5Cfrac%7BdE%7D%7BdB%5EL%7D%3D%5Csum_%7Bj%7D%5Cfrac%7BdE_j%7D%7Bd%7BZ_j%7D%5EL%7D)](#_)

A так же вычислим саму ошибку для отслеживания прогресса обучения нейросети. Для этого воспользуемся функцией CrossEntropy, она показывает, насколько распределение вероятности ответов соответствует действительности. CrossEntropy(P, Y) = -Σ<sub>j</sub> Y<sub>j</sub>\*logP<sub>j</sub>. Но так как в истинном распределении вероятности только одна единица и остальные нули, то при умножении останется только индекс правильного результата, от суммы останется только -logP<sub>Y</sub>.

После рассмотрения принципа обучения нейросети, нужно научиться делать это в меру, так как ИНС может переобучиться. Это значит, что она слишком хорошо запомнит тренировочные данные и станет хуже работать на тестовых данных. Оптимальные гиперпараметры для конкретной нейросети подбираются в ходе ее обучения по валидационным данным.



2 <a name="_toc190794894"></a>**ПРАКТИЧЕСКАЯ ЧАСТЬ**

**Задача:** реализовать полносвязный перцептрон на языке Python без использования заготовленных библиотек вроде TensorFlow, дающих возможность создавать нейронные сети без написания их логики работы. И сделать это парадигме ООП, то есть создать класс нейросети для дальнейшей возможности импортирования этого класса в другие программы и большей универсальности.

2.1 <a name="_toc190794895"></a>**Код модели нейронной сети**

При обучении, необходимо инициализировать веса и смещения случайными значениями, например, центрированными около нуля и со средним отклонением, равным квадратному корню из количества нейронов в слое.
```python
def init_weights(self, layers):
    self.weights = []  # Инициализация пустых весов
    self.bias = []

    # Присваивание случайных значений для всех слоев
    for num in range(len(layers) - 1):  
        self.weights.append(
            np.random.normal(
                0.0, pow(layers[num], -0.5), (layers[num], layers[num + 1])
                )
            )
        self.bias.append(
            np.random.normal(
                0.0, pow(layers[num + 1], -0.5), (layers[num + 1])
                )
            )
```
И прежде, чем реализовывать обучение нейросети, нужно создать функции нужные для этого. А именно функцию активации, ее производную, функцию для перевода значений в вероятность, функцию ошибки, предсказание нейросети и функцию для ее тестирования.
```python
def ReLU(x):  # Функция активации
    return np.maximum(0, x)

def ReLU_deriv(Z):  # Производная ReLU
    return np.where(Z > 0, 1, 0)

def SoftMax_batch(vect):  # Преобразование батча к вероятностям
    exp_vect = np.exp(vect - np.max(vect, axis=1, keepdims=True))
    return exp_vect / np.sum(exp_vect, axis=1, keepdims=True)

# Вычисление расстояния между вероятностями
def cross_entropy_batch(p, y, epsilon=1e-12):
    p = np.clip(p, epsilon, 1.0)  # Защита от деления на 0
    return -np.sum(np.log(p[np.arange(len(y)), y])) / len(y)

# Преобразование вектора правильных ответов к матрице истинных вероятностей
def y_vector_batch(y, num_classes):
    return np.eye(num_classes)[y]

def num_correct(p, y): # Подсчет верных ответов
    return np.sum(np.argmax(p, axis=1) == y)

def predict_batch(self, data):  # Предсказание нейросети для батча
    for i in range(len(self.bias) - 1):
        data = ReLU(data @ self.weights[i] + self.bias[i])
    return SoftMax_batch(data @ self.weights[-1] + self.bias[-1])

def test(self, data, labels):
    return num_correct(self.predict_batch(data), labels) / data.shape[0] * 100

def num_correct(p, y): # Подсчет верных ответов
    return np.sum(np.argmax(p, axis=1) == y)
```
Функция обучения будет брать на вход данные для обучения и тестирования и гиперпараметры нейросети. После будет получать предсказание и сохранять его для дальнейшего использования. После этого по формулам будет получать векторы градиента и корректировать значения весов. После завершения каждой эпохи будет сохранять значения функции ошибки и аккуратности на тестовых данных.
```python
def learn(self, data, labels, epochs, batch_size, train_rate, valid_data, valid_labels):
    print('Начало обучения')
    for ep in range(epochs):
        epoch_loss = 0
        correct_count = 0

        indices = np.arange(data.shape[0])  # Перемешивание данных
        np.random.shuffle(indices)
        data = data[indices]
        labels = labels[indices]
        for i in range(data.shape[0] // batch_size):

            # Прямое распространение нейросети
            layer = [data[i * batch_size : (i + 1) * batch_size]]
            y = labels[i * batch_size : (i + 1) * batch_size]
            Z = []

            for L in range(len(self.weights) - 1):
                Z.append(layer[L] @ self.weights[L] + self.bias[L])
                layer.append(ReLU(Z[L]))
            layer.append(SoftMax_batch(layer[-1] @ self.weights[-1] + self.bias[-1]))

            # Обратное распространение
            dE_dZ = layer[-1] - y_vector_batch(y, self.weights[-1].shape[1])
            for L in range(len(self.weights) - 1, -1, -1):

                dE_dW = np.transpose(layer[L]) @ dE_dZ
                dE_dB = np.sum(dE_dZ, axis=0, keepdims=True)
                dE_dB.resize(self.bias[L].shape)
                if L > 0:
                    dE_dA = dE_dZ @ np.transpose(self.weights[L])
                    dE_dZ = dE_dA * ReLU_deriv(Z[L - 1])
                
                # Корректировка весов
                self.weights[L] -= train_rate * dE_dW
                self.bias[L] -= train_rate * dE_dB

            epoch_loss += np.sum(cross_entropy_batch(layer[-1], y))
            correct_count += num_correct(layer[-1], y)

        # Вывод номера эпохи и точности нейросети
        accuracy = correct_count / data.shape[0] * 100
        print(f'Epoch {ep+1}/{epochs}: Loss = {epoch_loss:.2f}, ' +
        f'Train accuracy = {accuracy:.2f}%, ' + 
        f'Valid accuracy = {self.test(valid_data, valid_labels):.2f}%')  
    print(f'Нейросеть обучена')
```

2.2 <a name="_toc190794896"></a>**Обучение нейронной сети**
2.2.1 <a name="_toc190794897"></a>**Эффективность работы**

Проверять эффективность обучения нейросети я буду используя набор данных MNIST. Это набор из 60000 тренировочных и 10000 тестовых черно-белых изображений рукописных цифр, написанных разными людьми в формате 28\*28 пикселей. Пример того, как выглядят изображения цифр: 
![Примеры изображений MNIST](https://i.postimg.cc/qBGknvH6/MNIST.png)

При обучении используется параметр — скорость обучения. Это коэффициент, на который умножается градиент. Можно предположить, что лучше оставить значение таким каким его дают формулы, но при таком способе нейросеть не может попасть в локальный минимум и в итоге не обучается: 
![График ошибки обучения со скоростью равной 1](https://i.postimg.cc/K8jvmSVk/1.jpg)
Так для набора данных MNIST и моей архитектуры нейросети, она начинает эффективно обучатся на значении скорости 0,01. Ниже это значение делать нет смысла, так как нейросеть будет слишком медленно обучатся.

Для обучения нейросети нужно выбрать количество слоев и нейронов в них. В первом слоев должно быть 784 нейрона так как пикселей в фотографии 28\*28. В последнем слое должно быть 10 нейронов, потому что выходной результат это распределение вероятностей того, что на изображение та или иная цифра. Размер батча обычно выбирается от 30 до 100, для того чтобы использовать меньше вычислительных ресурсов, так что я выберу 50 изображений для одного батча.

Опытным путем я выяснил, что большинство конфигураций нейронных сетей для распознавания цифр, перестают улучшать результат на 15 эпохе, а некоторые конфигурации достигают 100% точности на тренировочных данных, что не позволяет им расти дальше. Исходя из этого, и чтобы не дать нейросети переобучится использовать я буду значение в 20 эпох.

Рассмотрим результаты обучения нейросети с различными конфигурациями:

- Самая первая в мире нейросеть — однослойный перцептрон.

Реализуем его, запустив нейросеть с слоями состоящими из 784 и 10 нейронов. Графики обучения: 
![График ошибки при обучении однослойного](https://i.postimg.cc/xdbfbG99/image.png)
На них видно, что точность такого перцептрона достигает 92%. Для такой простой архитектуры это уже хороший результат. 

Так как первый слой сразу связывается с нейронами, дающими ответ, то если визуализировать матрицу связей первого слоя с 0-ым нейроном второго слоя, то можно увидеть усредненное изображение нуля из этого набора данных, где белым показаны самые большие веса, а черным самые маленькие и так же с остальными цифрами. 
![Визуализация весов связанных с ответом 0](https://i.postimg.cc/dtqVSTj9/0.png)

- Большой перцептрон.

Обучим сеть с слоями содержащими такое количество нейронов:  784, 512, 256, 10. Обучение такой нейросети длилось 5 минут, когда как однослойный перцептрон обучился меньше чем на минуту. Конечная точность этой конфигурации вышла 98.62% 
![График ошибки при обучении большого](https://i.postimg.cc/nhWVzSbd/image.png)

- Средняя конфигурация.

Выберем оптимальные параметры: 784, 256, 10. В таком случае модель обучается минуту, а 100% на тренировочных данных добивается еще быстрее. При этом ее точность почти такая же, что и у более крупной модели.
![График ошибки при обучении оптимального](https://i.postimg.cc/hPptFPCR/image.png)

При этом если визуализировать весы этой сети, то у первого слоя видно, что веса определяют отдельный элемент цифры, а на втором слое просто шум.
![Пример визуализации весов многослойного перцептрона](https://i.postimg.cc/C1GwcFZt/image.png)

Если посмотреть на каких изображениях ошиблась нейросеть, то можно понять, что у человека была бы тоже не 100% точность.
![Пример ошибки перцептрона](https://i.postimg.cc/cCT0NXBB/image.png)

2.2.2 <a name="_toc190794898"></a>**Скорость работы**

Для проверки гипотезы измерим время работы нейросети. Для этого засеку время с помощью стандартного модуля time и получу предсказание для тренировочных и тестовых данных. Учитывая, что я не располагаю мощной вычислительной техникой, результаты получились: 0.459 секунды для 60 тысяч тренировочных и 0.086 секунды для 10 тысяч тестовых изображений. Это подтверждает, что искусственный интеллект работает в тысячи раз быстрее человека.

#
<a name="_toc190794899"></a>**ЗАКЛЮЧЕНИЕ**

В ходе выполнения проекта «Искусственный интеллект. Модель, проектирование, разработка» была поставлена цель — изучить принципы работы нейронных сетей и создать собственную модель полносвязного перцептрона на языке Python без использования готовых библиотек. Эта цель была успешно достигнута.

В теоретической части проекта были рассмотрены:

- История развития нейронных сетей и ключевые понятия, связанные с искусственным интеллектом.
- Современные примеры искусственного интеллекта для применения в практике.
- Принципы работы полносвязного перцептрона, включая алгоритмы прямого и обратного распространения ошибки.

Практическая реализация включала разработку класса нейронной сети по принципам объектно-ориентированного программирования. Модель была протестирована на наборе данных MNIST, что позволило наглядно продемонстрировать, что даже простая архитектура способна достигать высокой точности распознавания рукописных цифр. Экспериментальные результаты (точность от 92% для однослойного перцептрона до 98+% для более сложных моделей) подтвердили эффективность выбранного подхода и корректность реализации.

Благодаря изученной информации и проделанной работе была создана страница проекта на GitHub, где размещен исходный код программы для обучения и работы нейросети, программы для визуализации весов нейросети, кода интерфейса позволяющего самостоятельно рисовать цифры и наглядно видеть результат работы нейросети, и весы обученной мною нейросети (Приложение №5, ссылка на проект).

**Теоретическая значимость:** работа рассказывает об устройстве нейронных сетей и поможет привлечь к этой теме больше людей.

**Практическая значимость проекта**:

- Проект служит отличной базой для дальнейшего изучения и экспериментов в области машинного обучения.
- Разработанный класс нейросети можно использовать в образовательных целях и как основу для создания более сложных моделей, а также может быть использована для обучения нейросети на различных наборах данных, включая MNIST, что демонстрирует возможности ИИ в решении задач классификации.

Таким образом, проделанная работа не только позволила глубже понять внутреннюю логику работы нейронных сетей, но и продемонстрировала, что самостоятельная реализация алгоритмов обучения — эффективный способ освоения основ искусственного интеллекта. Полученные знания и навыки станут прочной базой для дальнейших исследований и разработки более сложных систем в области ИИ.

Считаю, что цель проекта достигнута, поставленные задачи выполнены. Гипотеза о том, что нейронные сети могут автоматизировать задачи классификации данных и делать это лучше и быстрее людей подтверждена. В будущем планирую продолжить исследование в данном направление и создавать более сложные модели искусственного интеллекта.

**Искусственный интеллект может открыть дорогу к обществу изобилия, в котором необходимость работать исчезнет для большинства людей.**  

Илон Маск [[13](https://tass.ru/obschestvo/19190483)]

**У нас есть уникальная возможность улучшить жизнь с помощью ИИ. Мы все еще на начальном этапе, но я уже верю, что ИИ станет самым значительным изменением в нашей жизни.**

Глава компании OpenAI Сэм Альтман [[14](rbc.ru/technology_and_media/13/02/2025/67adc1e69a7947a9e82690d7)]
#
<a name="_toc190794900"></a>**БИБЛИОГРАФИЯ**

1. Вывод общих матричных формул обратного распространения ошибки. <https://habr.com/ru/companies/ods/articles/344116/>
1. Вывод формул для прямого распространения в перцептроне. [https://habr.com/ru/articles/722422/#anchor_3](https://habr.com/ru/articles/722422/%23anchor_3)
1. Информация о функции ReLU. <https://en.wikipedia.org/wiki/Rectifier_(neural_networks)>
1. История создания нейронных сетей. <https://habr.com/ru/articles/861888/>
1. Общее представление об обратном распространении ошибки и формулы. <https://en.wikipedia.org/wiki/Backpropagation>
1. Описание модели перцептрона. <https://habr.com/ru/articles/214317/>
1. Определение искусственного интеллекта. <https://ru.wikipedia.org/wiki/ИИ>
1. Определение нейронной сети. <https://ru.wikipedia.org/wiki/Нейронная_сеть>
1. Принципы прямого и обратного распространения ошибки. [https://education.yandex.ru/handbook/ml/article/pervoe-znakomstvo-s-polnosvyaznymi-nejrosetyami#forward-and-backward-propagation](https://education.yandex.ru/handbook/ml/article/pervoe-znakomstvo-s-polnosvyaznymi-nejrosetyami%23forward-and-backward-propagation)
1. Разновидности функций активации. <https://deepmachinelearning.ru/docs/Neural-networks/MLP/Activation-functions>
1. Теоретические объяснения логики перцептронов. <https://neural.radkopeter.ru/chapter/персептроны/>
1. Цитата В. В. Путина президента РФ про Искусственный интеллект. <https://tass.ru/ekonomika/22643091>
1. Цитата Илона Маска об ИИ. <https://tass.ru/obschestvo/19190483>
1. Цитата Сэма Альтмана об ИИ в нашей жизни. [rbc.ru/technology_and_media/13/02/2025/67adc1e69a7947a9e82690d7](https://www.rbc.ru/technology_and_media/13/02/2025/67adc1e69a7947a9e82690d7)